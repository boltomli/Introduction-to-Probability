%This chapter was modified on 4/2/97.%\setcounter{chapter}{6}\chapter[Sums of Random Variables]{Sums of Independent Random Variables}\label{chp 7}  \section{Sums of Discrete Random Variables}\label{sec 7.1} In this chapter we turn to the important question of determining thedistribution of a sum of independent random variables in terms of thedistributions of the individual constituents.  In this section we consider onlysums of discrete random variables, reserving the case of continuous randomvariables for the next section.We consider here only random variables whose values are integers.  Their distributionfunctions are then defined on these integers.  Weshall find it convenient to assume here that these distribution functions aredefined for \emx {all} integers, by defining them to be~0 where they are nototherwise defined.\subsection*{Convolutions}\parSuppose $X$ and $Y$ are two independent discrete random variables with distribution functions $m_1(x)$ and $m_2(x)$.  Let $Z = X+Y$.  We would like to determine the distribution function $m_3(x)$ of $Z$.  To do this, it is enough to determine the probability that $Z$ takes on the value $z$, where $z$ is an arbitrary integer.  Suppose that $X = k$, where $k$is some integer.  Then $Z = z$ if and only if $Y = z-k$.  So the event $Z = z$ is the unionof the pairwise disjoint events$$(X = k)\  \mbox{and\ }(Y = z-k)\ ,$$where $k$ runs over the integers.  Since these events are pairwise disjoint, we have$$P(Z = z) = \sum_{k = -\infty}^\infty P(X = k)\cdot P(Y = z - k)\ .$$Thus, we have found the distribution function of the random variable $Z$.  Thisleads to the following definition.\begin{definition}\label{defn 7.1}Let $X$ and $Y$ be two independent integer-valued random variables, withdistribution functions $m_1(x)$~and~$m_2(x)$ respectively.  Then the {\emconvolution}\index{convolution} of$m_1(x)$~and~$m_2(x)$ is the distribution function $m_3 = m_1*m_2$ given by$$ m_3(j) = \sum_k m_1(k) \cdot m_2(j - k)\ ,$$for $j = \ldots,\ -2,\ -1,\ 0,\ 1,\ 2,\ \ldots$.  The function $m_3(x)$ is the distributionfunction of the random variable $Z = X + Y$.  \end{definition}\parIt is easy to see that the convolution operation is commutative, and it is straightforward toshow that it is also associative.\parNow let $S_n = X_1 + X_2 +\cdots+ X_n$ be the sum of $n$independent random variables of an independent trials process with common distributionfunction $m$ defined on the integers.  Then the distribution function of $S_1$ is $m$.  We canwrite$$S_n = S_{n - 1} + X_n\ .$$Thus, since we know the distribution function of $X_n$ is $m$, we can find the distributionfunction of $S_n$ by induction.\begin{example}A die is rolled twice.  Let $X_1$ and $X_2$ be the outcomes, and let $S_2 = X_1+ X_2$ be the sum of these outcomes.  Then $X_1$ and $X_2$ have the commondistribution function:$$m = \pmatrix{1 & 2 & 3 & 4 & 5 & 6 \cr1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6\cr}.$$The distribution function of $S_2$ is then the convolution of this distribution with itself. Thus,\begin{eqnarray*}P(S_2 = 2) &=& m(1)m(1) \\           &=& \frac 16 \cdot \frac 16 = \frac 1{36}\ , \\P(S_2 = 3) &=& m(1)m(2) + m(2)m(1) \\           &=& \frac 16 \cdot \frac 16 + \frac 16 \cdot \frac 16 = \frac 2{36}\ ,\\P(S_2 = 4) &=& m(1)m(3) + m(2)m(2) + m(3)m(1) \\           &=& \frac 16 \cdot \frac 16 + \frac 16 \cdot \frac 16 + \frac 16\cdot \frac 16 = \frac 3{36}\ .\\\end{eqnarray*}Continuing in this way we would find $P(S_2 = 5) = 4/36$, $P(S_2 = 6) = 5/36$,$P(S_2 = 7) = 6/36$, $P(S_2 = 8) = 5/36$, $P(S_2 = 9) = 4/36$, $P(S_2 = 10) =3/36$, $P(S_2 = 11) = 2/36$, and $P(S_2 = 12) = 1/36$.The distribution for $S_3$ would then be the convolution of the distribution for $S_2$with the distribution for $X_3$.  Thus\begin{eqnarray*}P(S_3 = 3) &=& P(S_2 = 2)P(X_3 = 1) \\           &=& \frac 1{36} \cdot \frac 16 = \frac 1{216}\ , \\P(S_3 = 4) &=& P(S_2 = 3)P(X_3 = 1) + P(S_2 = 2)P(X_3 = 2) \\           &=& \frac 2{36} \cdot \frac 16 + \frac 1{36} \cdot \frac 16 = \frac3{216}\ ,\\\end{eqnarray*}and so forth.\parThis is clearly a tedious job, and a program should be written to carry out this calculation. To do this we first write a program to form the convolution of two densities $p$~and~$q$ andreturn the density $r$.  We can then write a program to findthe density for the sum $S_n$ of $n$ independent random variables with a commondensity $p$, at least in the case that the random variables have a finite number of possiblevalues.\putfig{3.5truein}{PSfig7-1}{Density of $S_n$ for rolling a die $n$ times.}{fig 7.1} \parRunning this program for the example ofrolling a die $n$ times for~$n = 10,\ 20,\ 30$ results in the distributions shown inFigure~\ref{fig 7.1}.  We see that, as in the case of Bernoulli trials, the distributionsbecome bell-shaped.  We shall discuss in Chapter~\ref{chp 9} a very general theorem calledthe {\em Central Limit Theorem} that will explain this phenomenon.\end{example}\begin{example}A well-known method for evaluating a bridge\index{bridge} hand is: an ace is assigned a valueof~4, a king~3, a queen~2, and a jack~1.  All other cards are assigned a valueof~0.  The \emx {point count}\index{point count} of the hand is then the sum of the values of thecards in the hand.  (It is actually more complicated than this, taking intoaccount voids in suits, and so forth, but we consider here this simplified formof the point count.)  If a card is dealt at random to a player, then the pointcount for this card has distribution$$p_X = \pmatrix{0 & 1 & 2 & 3 & 4 \cr36/52 & 4/52 & 4/52 & 4/52 & 4/52\cr}.$$Let us regard the total hand of 13~cards as 13 independent trials with thiscommon distribution.  (Again this is not quite correct because we assume herethat we are always choosing a card from a full deck.)  Then the distribution for thepoint count $C$ for the hand can be found from the program {\bfNFoldConvolution}\index{NFoldConvolution (program)} by using the distribution for a singlecard and choosing$n = 13$.  A player with a point count of~13 or more is said to have an  \emx {opening bid.} The probability of having an opening bid is then$$P(C \geq 13)\ .$$Since we have the distribution of $C$, it is easy to compute this probability.  Doing this wefind that$$P(C \geq 13) = .2845\ ,$$so that about one in four hands should be an opening bid according to thissimplified model.  A more realistic discussion of this problem can be found inEpstein,\index{EPSTEIN, R.}  \emx {The Theory of Gambling and Statistical Logic.}\footnote{R.~A.Epstein,  \emx {The Theory of Gambling and Statistical Logic,} rev.~ed.\ (NewYork: Academic Press, 1977).}\end{example}\parFor certain special distributions it is possible to find an expression for thedistribution that results from convoluting the distribution with itself $n$~times.\parThe convolution of two binomial distributions,\index{convolution!of binomial distributions} one withparameters$m$ and$p$ and the other with parameters $n$ and $p$, is a binomial distribution with parameters $(m+n)$and$p$.  This fact follows easily from a consideration of the experiment which consists of firsttossing a coin $m$ times, and then tossing it $n$ more times.\parThe convolution of $k$ geometric distributions\index{convolution!of geometric distributions} withcommon parameter$p$ is a negative binomial distribution with parameters $p$ and $k$.  This can be seen byconsidering the experiment which consists of tossing a coin until the $k$th head appears.\exercises \begin{LJSItem}\i\label{exer 7.1.1} A die is rolled three times.  Find the probability that the sum ofthe outcomes is\begin{enumerate}\item greater than 9.\item an odd number.\end{enumerate}\i\label{exer 7.1.2} The price of a stock on a given trading day changes according to thedistribution$$p_X = \pmatrix{-1 & 0 & 1 & 2 \cr1/4 & 1/2 & 1/8 & 1/8\cr}.$$Find the distribution for the change in stock price after two (independent) tradingdays.\i\label{exer 7.1.3} Let $X_1$ and $X_2$ be independent random variables with commondistribution$$p_X = \pmatrix{0 & 1 & 2 \cr1/8 & 3/8 & 1/2\cr}.$$Find the distribution of the sum $X_1 + X_2$.\i\label{exer 7.1.4} In one play of a certain game you win an amount $X$ with distribution$$p_X = \pmatrix{1 & 2 & 3 \cr1/4 & 1/4 & 1/2\cr}.$$Using the program {\bf NFoldConvolution} find the distribution for your total winningsafter ten (independent) plays.  Plot this distribution.\i\label{exer 7.1.5} Consider the following two experiments: the first has outcome $X$ takingon the values~0,~1, and~2 with equal probabilities; the second results in an(independent) outcome $Y$ taking on the value~3 with probability 1/4 and~4with probability 3/4.  Find the distribution of\begin{enumerate}\item $Y + X$.\item $Y - X$.\end{enumerate}\i\label{exer 7.1.6} People arrive at a queue according to the following scheme: During each minute of time either 0~or~1 person arrives.  The probability that 1 personarrives is $p$ and that no person arrives is $q = 1 - p$.  Let $C_r$ be thenumber of customers arriving in the first $r$~minutes.  Consider a Bernoullitrials process with a success if a person arrives in a unit time and failureif no person arrives in a unit time.  Let $T_r$ be the number of failuresbefore the $r$th success.\begin{enumerate}\item What is the distribution for $T_r$?\item What is the distribution for $C_r$?\item Find the mean and variance for the number of customers arriving in thefirst $r$ minutes.\end{enumerate}\i\label{exer 7.1.7} \begin{enumerate}\item A die is rolled three times with outcomes $X_1$,~$X_2$, and~$X_3$.  Let$Y_3$ be the maximum of the values obtained.  Show that$$P(Y_3 \leq j) = P(X_1 \leq j)^3\ .$$Use this to find the distribution of~$Y_3$.  Does $Y_3$ have a bell-shaped distribution?\itemNow let $Y_n$ be the maximum value when $n$ dice are rolled.  Find the distribution of$Y_n$.  Is this distribution bell-shaped for large values of $n$?\end{enumerate}\i\label{exer 7.1.10} A baseball player is to play in the World Series.  Based upon his seasonplay, you estimate that if he comes to bat four times in a game the number ofhits he will get has a distribution$$p_X = \pmatrix{0 & 1 & 2 & 3 & 4 \cr.4 & .2 & .2 & .1 & .1\cr}.$$Assume that the player comes to bat four times in each game of the series. \begin{enumerate}\itemLet $X$ denote the number of hits that he gets in a series.  Using the program {\bfNFoldConvolution}, find the distribution of $X$ for each of the possible series lengths:four-game, five-game, six-game, seven-game. \itemUsing one of the distribution found in part (a), find the probability that his batting averageexceeds .400 in a four-game series.  (The batting average is the number of hits divided by thenumber of times at bat.)\item Given the distribution $p_X$, what is his long-term batting average?\end{enumerate}\i\label{exer 7.1.11} Prove that you cannot load two dice in such a way thatthe probabilities for any sum from 2~to~12 are the same.  (Be sure to considerthe case where one or more sides turn up with probability zero.)\i\label{exer 7.1.12} (L\'evy\footnote{See M. Krasner and B. Ranulae, ``Sur une Propriet\'edes Polynomes de la Division du Circle"; and the following note by J.Hadamard, in  \emx {C.\ R.\ Acad.\ Sci.,} vol.~204 (1937), pp.~397--399.}) Assumethat $n$ is an integer, not prime.  Show that you can find two distributions$a$~and~$b$ on the nonnegative integers such that the convolution of$a$~and~$b$ is the equiprobable distribution on the set 0, 1, 2, \dots, $n - 1$.  If$n$ is prime this is not possible, but the proof is not so easy.  (Assume thatneither $a$~nor~$b$ is concentrated at 0.)\i\label{exer 7.1.13} Assume that you are playing craps with dice that are loaded in thefollowing way: faces two, three, four, and five all come up with the sameprobability $(1/6) + r$.  Faces one and six come up with probability $(1/6) - 2r$,with $0 < r < .02$.  Write a computer program to find the probability ofwinning at craps with these dice, and using your program find which valuesof~$r$ make craps a favorable game for the player with these dice.\end{LJSItem}\choice{}{\section{Sums of Continuous Random Variables}\label{sec 7.2}In this section we consider the continuous version of the problem posed in theprevious section: How are sums of independent random variables distributed?\subsection*{Convolutions}\begin{definition} Let $X$ and $Y$ be two continuous random variables with densityfunctions $f(x)$ and $g(y)$, respectively.  Assume that both $f(x)$ and $g(y)$ are defined forall real numbers.  Then the  \emx {convolution}\index{convolution} $f*g$ of$f$~and~$g$ is the function given by\begin{eqnarray*} (f*g)(z) &=& \int_{-\infty}^{+\infty} f(z - y) g(y)\,dy \\         &=& \int_{-\infty}^{+\infty} g(z - x) f(x)\, dx\ .\end{eqnarray*} \end{definition} This definition is analogous to the definition, given in Section~\ref{sec 7.1}, of theconvolution of two distribution functions.  Thus it should not be surprising that if $X$ and$Y$ are independent, then the density of their sum is the convolution of their densities. This fact is stated as a theorem below, and its proof is left as an exercise (seeExercise~\ref{exer 7.2.0.5}).  \begin{theorem} Let $X$ and $Y$ be two independent random variables with density functions$f_X(x)$ and $f_Y(y)$ defined for all~$x$.  Then the sum $Z = X + Y$ is a random variable withdensity function $f_Z(z)$, where $f_Z$ is the convolution of $f_X$~and~$f_Y$.\end{theorem}\parTo get a better understanding of this important result, we will look at some examples.\pagebreak[4]\subsection*{Sum of Two Independent Uniform Random Variables}\begin{example}\label{exam 7.6}Suppose we choose independently two numbers at random from the interval$[0,1]$ with uniform probability density\index{convolution!of uniform densities}.  What is thedensity of their sum?Let $X$ and $Y$ be random variables describing our choices and $Z = X + Y$their sum.  Then we have$$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                               1 & \;\mbox{if $0 \leq x \leq 1$,} \\                               0 & \;\mbox{otherwise;}                  \end{array}         \right. $$and the density function for the sum is given by$$f_Z(z) = \int_{-\infty}^{+\infty} f_X(z - y) f_Y(y)\,dy\ .$$Since $f_Y(y) = 1$ if $0 \leq y \leq 1$ and 0 otherwise, this becomes$$f_Z(z) = \int_0^1 f_X(z - y)\,dy\ .$$Now the integrand is 0 unless $0 \leq z - y \leq 1$ (i.e., unless $z - 1 \leq y\leq z$) and then it is~1.  So if $0 \leq z \leq 1$, we have$$f_Z(z) = \int_0^z \, dy = z\ ,$$while if $1 < z \leq 2$, we have$$f_Z(z) = \int_{z - 1}^1\, dy = 2 - z\ ,$$and if $z < 0$ or $z > 2$ we have $f_Z(z) = 0$ (see Figure~\ref{fig 7.5}).  Hence,$$f_Z(z) = \left \{ \begin{array}{ll}                               z,          & \;\mbox{if $0 \leq z \leq 1,$} \\                               2-z,        & \;\mbox{if $1 < z \leq 2,$} \\                               0,          & \;\mbox{otherwise.}\end{array}\right. $$\putfig{3.5truein}{PSfig7-5}{Convolution of two uniform densities.}{fig 7.5} %4.5trueinNote that this result agrees with that of Example~\ref{exam 2.1.4.5}.\end{example}\subsection*{Sum of Two Independent Exponential Random Variables}\begin{example}\label{exam 7.7}Suppose we choose two numbers at random from the interval $[0,\infty)$ withan  \emx {exponential} density\index{convolution!of exponential densities} with parameter $\lambda$. What is the density of their sum?Let $X$, $Y$, and $Z = X + Y$ denote the relevant random variables, and$f_X$,~$f_Y$, and~$f_Z$ their densities.  Then$$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}           \lambda e^{-\lambda x}, & \;\mbox{if $x \geq 0$},\\                                 0, & \;\mbox{otherwise;}                  \end{array}         \right. $$and so, if $z > 0$,\begin{eqnarray*}f_Z(z) &=& \int_{-\infty}^{+\infty} f_X(z - y) f_Y(y)\, dy \\       &=& \int_0^z \lambda e^{-\lambda(z - y)} \lambda e^{-\lambda y}\, dy \\       &=& \int_0^z \lambda^2 e^{-\lambda z}\, dy \\       &=& \lambda^2 z e^{-\lambda z},\\\end{eqnarray*}while if $z < 0$, $f_Z(z) = 0$ (see Figure~\ref{fig 7.6}).  Hence,\putfig{3.5truein}{PSfig7-6}{Convolution of two exponential densities with $\lambda = 1$.}{fig 7.6} $$f_Z(z) = \left \{ \begin{array}{ll}                       \lambda^2 z e^{-\lambda z},                                           & \;\mbox{if $z \geq 0$},\\                                      0,  & \;\mbox{otherwise.}                  \end{array}         \right. $$\end{example}\subsection*{Sum of Two Independent Normal Random Variables}\begin{example}\label{exam 7.8}It is an interesting and important fact that the convolution of two normal densities with means $\mu_1$~and~$\mu_2$ and variances $\sigma_1$~and~$\sigma_2$ is again a normal density, with mean $\mu_1 + \mu_2$ and variance $\sigma_1^2 + \sigma_2^2$.We will show this in the special case that both random variables are standard normal.The general case can be done in the same way, but the calculation is messier.  Anotherway to show the general result is given in Example~\ref{exam 10.3.3}.\parSuppose $X$ and $Y$ are two independent random variables, each with the standard {\emnormal}\index{convolution!of normal densities} density (see Example~\ref{exam 5.16}).  We have$$f_X(x) = f_Y(y) = \frac 1{\sqrt{2\pi}} e^{-x^2/2}\ ,$$and so\begin{eqnarray*}f_Z(z) &=& f_X * f_Y(z) \\&=& \frac 1{2\pi} \int_{-\infty}^{+\infty} e^{-(z -y)^2/2} e^{-y^2/2}\, dy \\       &=& \frac 1{2\pi} e^{-z^2/4} \int_{-\infty}^{+\infty} e^{-(y - z/2)^2}\,dy \\       &=& \frac 1{2\pi} e^{-z^2/4}\sqrt {\pi} \biggl[\frac 1{\sqrt {\pi}}\int_{-\infty}^\inftye^{-(y-z/2)^2}\,dy\ \biggr]\ .\\\end{eqnarray*}The expression in the brackets equals 1, since it is the integral of the normal densityfunction with $\mu = 0$ and $\sigma = \sqrt 2$.  So, we have$$f_Z(z) = \frac 1{\sqrt{4\pi}} e^{-z^2/4}\ .$$\end{example}\subsection*{Sum of Two Independent Cauchy Random Variables}\begin{example}\label{exam 7.9}Choose two numbers at random from the interval$(-\infty,+\infty)$ with the Cauchy\index{convolution!of Cauchy densities} density with parameter $a = 1$ (see Example~\ref{exam 5.20}).  Then $$f_X(x) = f_Y(x) = \frac 1{\pi(1 + x^2)}\ ,$$and $Z = X + Y$ has density$$f_Z(z) = \frac 1{\pi^2} \int_{-\infty}^{+\infty} \frac {1}{1 + (z - y)^2} \frac{1}{1 + y^2} \, dy\ .$$This integral requires some effort, and we give here only the result(see Section~\ref{sec 10.3}, or Dwass\footnote{M. Dwass, ``On the Convolution of CauchyDistributions,"  \emx {American Mathematical Monthly,} vol.~92, no.~1, (1985),pp.~55--57; see also R.~Nelson, letters to the Editor, ibid., p.~679.}):$$f_Z(z) = \frac {2}{\pi(4 + z^2)}\ .$$\parNow, suppose that we ask for the density function of the \emx {average} $$A = (1/2)(X + Y)$$ of $X$~and~$Y$.  Then $A = (1/2)Z$.  Exercise~\ref{sec 5.2}.\ref{exer5.2.18} shows that if $U$ and $V$ are two continuous random variables  with density functions$f_U(x)$ and $f_V(x)$, respectively, and if $V = aU$, then $$f_V(x) = \biggl(\frac 1a\biggr)f_U\biggl(\frac xa\biggr)\ .$$Thus, we have$$f_A(z) = 2f_Z(2z) = \frac 1{\pi(1 + z^2)}\ .$$Hence, the density function for the average of two random variables, eachhaving a Cauchy density, is again a random variable with a Cauchy density; thisremarkable property is a peculiarity of the Cauchy density.  One consequence ofthis is if the error in a certain measurement process had a Cauchydensity and you averaged a number of measurements, the average could not beexpected to be any more accurate than any one of your individual measurements!\end{example}\subsection*{Rayleigh Density}\index{density function!Rayleigh}\index{Rayleighdensity}\begin{example}\label{exam 7.10}Suppose $X$ and $Y$ are two independent standard normal random variables. Now suppose we locate a point~$P$ in the $xy$-plane with coordinates $(X,Y)$ andask: What is the density of the square of the distance of~$P$ from the origin? (We have already simulated this problem in Example~\ref{exam 5.19}.)  Here, with the precedingnotation, we have$$f_X(x) = f_Y(x) = \frac 1{\sqrt{2\pi}} e^{-x^2/2}\ .$$Moreover, if $X^2$ denotes the square of $X$, then (see Theorem~\ref{thm 5.1} and thediscussion following) \begin{eqnarray*}f_{X^2}(r) &=& \left \{ \begin{array}{ll}        \frac{1}{2\sqrt r} (f_X(\sqrt r) + f_X(-\sqrt r))  & \;\mbox{if $r > 0,$} \\                                     0                     & \;\mbox{otherwise.}\end{array}\right. \\           &=& \left \{ \begin{array}{ll}        \frac{1}{\sqrt {2 \pi r}} (e^{-r/2}) \hspace{.8in} & \;\mbox{if $r > 0,$} \\                                       0                   & \;\mbox{otherwise.}\end{array}\right. \\\end{eqnarray*}This is a gamma density with $\lambda = 1/2$, $\beta = 1/2$ (seeExample~\ref{exam 7.7}).  Now let $R^2 = X^2 + Y^2$.  Then\begin{eqnarray*}f_{R^2}(r) &=& \int_{-\infty}^{+\infty} f_{X^2}(r - s) f_{Y^2}(s)\, ds \\           &=& \frac 1{4\pi} \int_{-\infty}^{+\infty} e^{-(r - s)/2} {\frac{r-s}{2}}^{-1/2} e^{-s} {\frac{s}{2}}^{-1/2}\, ds\ , \\           &=& \left \{\begin{array}{ll}                       {\frac {1}{2}} e^{-r^2/2}, & \;\mbox{if $r \geq 0,$} \\                               0,                 & \;\mbox{otherwise.}                  \end{array}         \right. \end{eqnarray*}Hence, $R^2$ has a gamma density with $\lambda = 1/2$, $\beta = 1$.  We caninterpret this result as giving the density for the square of the distanceof~$P$ from the center of a target if its coordinates are normally distributed.The density of the random variable $R$ is obtained from that of $R^2$ in theusual way (see Theorem~\ref{thm 5.1}), and we find$$f_R(r) = \left \{ \begin{array}{ll}                       \frac 12 e^{-r^2/2} \cdot 2r = re^{-r^2/2},                                           & \;\mbox{if $r \geq 0,$} \\                               0,         & \;\mbox{otherwise.}                  \end{array}         \right. $$Physicists will recognize this as a Rayleigh density.  Ourresult here agrees with our simulation in Example~\ref{exam 5.19}.\end{example}\subsection*{Chi-Squared Density}\index{chi-squared density}\index{density function!chi-squared}More generally, the same method shows that the sum of the squares of~$n$independent normally distributed random variables with mean~0 and standarddeviation~1 has a gamma density with $\lambda = 1/2$ and $\beta = n/2$.  Such adensity is called a  \emx {chi-squared density} with $n$ degrees of freedom.  Thisdensity was introduced in Chapter~\ref{chp 5}.  In Example~\ref{exam 5.20}, weused this density to test the hypothesis that two traits were independent.  \parAnother important use of the chi-squared density is in comparing experimental datawith a theoretical discrete distribution, to see whether the data supports the theoretical model.  More specifically, suppose that we have an experiment with afinite set of outcomes.  If the set of outcomes is countable, we group them intofinitely many sets of outcomes.  We propose a theoretical distribution which we thinkwill model the experiment well.  We obtain some data by repeating the  experiment anumber of times.  Now we wish to check how well the theoretical distribution fits thedata.\parLet $X$ be the random variable which represents a theoretical outcome in the modelof the experiment, and let $m(x)$ be the distribution function of $X$.  In a mannersimilar to what was done in Example~\ref{exam 5.20}, we calculate the value of theexpression$$V = \sum_x \frac{(o_x - n \cdot m(x))^2}{n \cdot m(x)}\ ,$$where the sum runs over all possible outcomes $x$, $n$ is the number of datapoints, and $o_x$ denotes the number of outcomes of type $x$ observedin the data.  Then for moderate or large values of $n$, the quantity $V$ isapproximately chi-squared distributed, with $\nu - 1$ degrees of freedom, where$\nu$ represents the number of possible outcomes.  The proof of this is beyond thescope of this book, but we will illustrate the reasonableness of this statement inthe next example.  If the value of $V$ is very large, when compared with theappropriate chi-squared density function, then we would tend to reject the hypothesisthat the model is an appropriate one for the experiment at hand.  We now give anexample of this procedure.\begin{example}Suppose we are given a single die.  We wish to test the hypothesis that the die isfair.  Thus, our theoretical distribution is the uniform distribution on the integersbetween 1 and 6.  So, if we roll the die $n$ times, the expected number of datapoints of each type is $n/6$.  Thus, if $o_i$ denotes the actual number of datapoints of type $i$, for $1 \le i \le 6$, then the expression$$V = \sum_{i = 1}^6 \frac{(o_i - n/6)^2}{n/6}$$is approximately chi-squared distributed with 5 degrees of freedom.\parNow suppose that we actually roll the die 60 times and obtain the data in Table~\ref{table 7.1}.\begin{table}\centering\begin{tabular}{|c|c|}\hline Outcome & Observed Frequency \\\hline 1 & 15\\\hline 2 & \hspace{.08in}8 \\\hline 3 & \hspace{.08in}7 \\\hline 4 & \hspace{.08in}5 \\\hline 5 & \hspace{.08in}7 \\\hline 6 & 18 \\\hline\end{tabular}\caption{Observed data.}\label{table 7.1}\end{table}If we calculate $V$ for this data, we obtain the value 13.6.  The graph of the chi-squared density with 5 degrees of freedom is shown in Figure~\ref{fig 7.7}.  Onesees that values as large as 13.6 are rarely taken on by $V$ if the die is fair, so wewould reject the hypothesis that the die is fair.   (When using this test, astatistician will reject the hypothesis if the data gives a value of $V$ which islarger than 95\% of the values one would expect to obtain if the hypothesis is true.)\putfig{3.5truein}{PSfig7-7}{Chi-squared density with 5 degrees of freedom.}{fig 7.7} \parIn Figure~\ref{fig 7.8}, we show the results of rolling a die 60 times, thencalculating $V$, and then repeating this experiment 1000 times.  The program that performsthese calculations is called {\bf DieTest}.\index{DieTest (program)}  Wehave  superimposed the chi-squared density with 5 degrees of freedom; one can seethat the data values fit the curve fairly well, which supports the statementthat the chi-squared density is the correct one to use. \putfig{4.5truein}{PSfig7-8}{Rolling a fair die.}{fig 7.8} \end{example}So far we have looked at several important special cases for which theconvolution integral can be evaluated explicitly.  In general, the convolutionof two continuous densities cannot be evaluated explicitly, and we must resortto numerical methods.  Fortunately, these prove to be remarkably effective, atleast for bounded densities.\subsection*{Independent Trials}We now consider briefly the distribution of the sum of~$n$ independent randomvariables, all having the same density function.  If $X_1$,~$X_2$, \dots,~$X_n$are these random variables and $S_n = X_1 + X_2 +\cdots+ X_n$ is their sum, then wewill have$$f_{S_n}(x) = \left( f_{X_1} * f_{X_2} *\cdots* f_{X_n} \right)(x)\ ,$$where the right-hand side is an $n$-fold convolution.  It is possible tocalculate this density for general values of~$n$ in certain simple cases.\begin{example}\label{exam 7.12}Suppose the $X_i$ are uniformly distributed\index{convolution!of uniform densities} on the interval$[0,1]$.  Then$$f_{X_i}(x) = \left \{ \begin{array}{ll}                         1,         & \;\mbox{if $0 \leq x \leq 1,$} \\                         0,         & \;\mbox{otherwise,}                   \end{array}          \right. $$and $f_{S_n}(x)$ is given by the formula\index{USPENSKY, J. B.}\footnote{J.~B. Uspensky,{\em Introduction to Mathematical Probability} (New York: McGraw-Hill, 1937),p.~277.}$$f_{S_n}(x) = \left \{ \begin{array}{ll}                         \frac 1{(n - 1)!} \sum_{0 \leq j \leq x} (-1)^j { n \choose j} (x - j)^{n - 1},     & \;\mbox{if $0 < x < n,$} \\                                0,          & \;\mbox{otherwise.}                   \end{array}          \right. $$The density $f_{S_n}(x)$ for~$n = 2$,~4, 6, 8,~10 is shown in Figure~\ref{fig 7.9}.\putfig{4.5truein}{PSfig7-9}{Convolution of $n$ uniform densities.}{fig 7.9} If the $X_i$ are distributed normally,\index{convolution!of standard normal densities} with mean~0and variance~1, then (cf.~Example~\ref{exam 7.8})$$f_{X_i}(x) = \frac 1{\sqrt{2\pi}} e^{-x^2/2}\ ,$$and$$f_{S_n}(x) = \frac 1{\sqrt{2\pi n}} e^{-x^2/2n}\ .$$Here the density $f_{S_n}$ for~$n = 5$,~10, 15, 20,~25 is shown in Figure~\ref{fig 7.10}.\putfig{4.5truein}{PSfig7-10}{Convolution of $n$ standard normal densities.}{fig 7.10} If the $X_i$ are all exponentially distributed,\index{convolution!of exponential densities}with mean~$1/\lambda$, then$$f_{X_i}(x) = \lambda e^{-\lambda x}\ ,$$and$$f_{S_n}(x) = \frac {\lambda e^{-\lambda x}(\lambda x)^{n - 1}}{(n - 1)!}\ .$$In this case the density $f_{S_n}$ for~$n = 2$,~4, 6, 8,~10 is shown inFigure~\ref{fig 7.11}.\putfig{4.5truein}{PSfig7-11}{Convolution of $n$ exponential densities with $\lambda = 1$.}{fig 7.11} \end{example}\exercises\begin{LJSItem}\i\label{exer 7.2.0.5}  Let $X$ and $Y$ be independent real-valued random variables with density functions $f_X(x)$ and $f_Y(y)$, respectively.  Show that the density function of the sum $X + Y$ is the convolution of the functions $f_X(x)$ and $f_Y(y)$.\emx {Hint}:  Let $\bar X$ be the joint random variable $(X, Y)$.  Then the jointdensity function of $\bar X$ is $f_X(x)f_Y(y)$, since $X$ and $Y$ are independent.  Nowcompute the probability that $X+Y \le z$, by integrating the joint density function over theappropriate region in the plane.  This gives the cumulative distribution function of $Z$. Now differentiate this function with respect to $z$ to obtain the density function of $z$.\i\label{exer 7.2.1} Let $X$ and $Y$ be independent random variables definedon the space $\Omega$, with density functions $f_X$~and~$f_Y$, respectively. Suppose that $Z = X + Y$.  Find the density $f_Z$ of~$Z$ if\begin{enumerate}\item $$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                              1/2,   & \;\mbox{if $-1 \leq x \leq +1,$} \\                              0,     & \;\mbox{otherwise.}                  \end{array}         \right. $$ \item $$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                              1/2,   & \;\mbox{if $3 \leq x \leq 5,$} \\                              0,     & \;\mbox{otherwise.}                  \end{array}         \right. $$\item $$f_X(x) = \left \{ \begin{array}{ll}                              1/2,   & \;\mbox{if $-1 \leq x \leq 1,$} \\                              0,     & \;\mbox{otherwise.}                  \end{array}         \right. $$\smallskip$$f_Y(x) = \left \{ \begin{array}{ll}                              1/2,   & \;\mbox{if $3 \leq x \leq 5,$} \\                              0,     & \;\mbox{otherwise.}                  \end{array}         \right. $$\item What can you say about the set $E = \{\,z : f_Z(z)> 0\,\}$ in eachcase?\end{enumerate}\i\label{exer 7.2.2} Suppose again that $Z = X + Y$.  Find $f_Z$ if\begin{enumerate}\item $$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                     x/2, & \mbox{if $0 < x < 2,$} \\                      0, & \mbox{otherwise}.                  \end{array}         \right. $$\item $$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                  (1/2)(x - 3), & \mbox{if $3 < x < 5,$} \\                            0, & \mbox{otherwise}.                   \end{array}         \right. $$\item $$f_X(x) = \left \{ \begin{array}{ll}                        1/2,    & \mbox{if $0 < x < 2,$} \\                            0, & \mbox{otherwise},                  \end{array}         \right. $$\smallskip $$f_Y(x) = \left \{ \begin{array}{ll}                           x/2, & \mbox{if $0 < x < 2,$} \\                            0, & \mbox{otherwise}.                  \end{array}         \right. $$\item What can you say about the set $E = \{\,z : f_Z(z)> 0\,\}$ in eachcase?\end{enumerate}\i\label{exer 7.2.3} Let $X$, $Y$, and $Z$ be independent random variableswith $$f_X(x) = f_Y(x) = f_Z(x)  = \left \{ \begin{array}{ll}                              1,     & \mbox{if $0 < x < 1,$} \\                              0,     & \mbox{otherwise.}                  \end{array}         \right. $$\noindent Suppose that $W = X + Y + Z$.  Find $f_W$ directly, and compare your answerwith that given by the formula in Example~\ref{exam 7.12}.   \emx {Hint}:  See Example~\ref{exam 7.6}.\i\label{exer 7.2.3.5} Suppose that $X$ and $Y$ are independent and $Z = X + Y$.  Find$f_Z$ if\begin{enumerate}\item $$f_X(x) = \left \{ \begin{array}{ll}                              \lambda e^{-\lambda x}, & \mbox{if $x > 0,$} \\                               0,                     & \mbox{otherwise.}                  \end{array}         \right. $$\smallskip$$f_Y(x) = \left \{ \begin{array}{ll}                              \mu e^{-\mu x}, & \mbox{if $x > 0,$} \\                              0,              & \mbox{otherwise.}                  \end{array}         \right. $$\item $$\ \ \ f_X(x) = \left \{ \begin{array}{ll}                              \lambda e^{-\lambda x}, & \mbox{if $x > 0,$} \\                              0,                      & \mbox{otherwise.}                  \end{array}         \right. $$\smallskip$$f_Y(x) = \left \{ \begin{array}{ll}                    1, & \mbox{if $0 < x < 1,$} \\                    0, & \mbox{otherwise.}                  \end{array}         \right. $$\end{enumerate}\i\label{exer 7.2.100} Suppose again that $Z = X + Y$.  Find $f_Z$ if\begin{eqnarray*}f_X(x) &=& \frac 1{\sqrt{2\pi}\sigma_1} e^{-(x - \mu_1)^2/2\sigma_1^2} \\f_Y(x) &=& \frac 1{\sqrt{2\pi}\sigma_2} e^{-(x - \mu_2)^2/2\sigma_2^2}\ .\end{eqnarray*}%********The next problem is too hard.\istar\label{exer 7.2.101} Suppose that $R^2 = X^2 + Y^2$.  Find $f_{R^2}$ and $f_R$ if\begin{eqnarray*}f_X(x) &=& \frac 1{\sqrt{2\pi}\sigma_1} e^{-(x - \mu_1)^2/2\sigma_1^2} \\f_Y(x) &=& \frac 1{\sqrt{2\pi}\sigma_2} e^{-(x - \mu_2)^2/2\sigma_2^2}\ .\end{eqnarray*}\i\label{exer 7.2.101.5} Suppose that $R^2 = X^2 + Y^2$.  Find $f_{R^2}$ and $f_R$ if$$f_X(x) = f_Y(x) = \left \{ \begin{array}{ll}                    1/2, & \mbox{if $-1 \leq x \leq 1,$} \\                    0,   & \mbox{otherwise.}                  \end{array}         \right. $$\i\label{exer 7.2.102} Assume that the service time for a customer at a bank isexponentially distributed with mean service time 2~minutes.  Let $X$ be the total servicetime for 10 customers.  Estimate the probability that $X > 22$ minutes.\i\label{exer 7.2.9} Let $X_1$,~$X_2$, \dots,~$X_n$ be $n$ independent randomvariables each of which has an exponential density with mean~$\mu$.  Let $M$ bethe \emx {minimum} value of the $X_j$.  Show that the density for~$M$ isexponential with mean $\mu/n$.   \emx {Hint}:  Use cumulative distribution functions.\i\label{exer 7.2.103} A company buys 100 lightbulbs, each of which has an exponentiallifetime of 1000 hours.  What is the expected time for the first of these bulbs to burnout?  (See Exercise~\ref{exer 7.2.9}.)\i\label{exer 7.2.104} An insurance company assumes that the time between claims fromeach of its homeowners' policies is exponentially distributed with mean~$\mu$.  Itwould like to estimate $\mu$ by averaging the times for a number of policies,but this is not very practical since the time between claims is about30~years.  At Galambos'\index{GALAMBOS, J.}\footnote{J. Galambos,  \emx {IntroductoryProbability Theory} (New York: Marcel Dekker, 1984), p.~159.} suggestion the company putsits customers in groups of~50 and observes the time of the first claim withineach group.  Show that this provides a practical way to estimate the valueof~$\mu$.\i\label{exer 7.2.105} Particles are subject to collisions that cause them to split intotwo parts with each part a fraction of the parent.  Suppose that this fraction isuniformly distributed between 0~and~1.  Following a single particle throughseveral splittings we obtain a fraction of the original particle $Z_n = X_1\cdot X_2 \cdot\dots\cdot X_n$ where each $X_j$ is uniformly distributedbetween 0~and~1.  Show that the density for the random variable $Z_n$ is$$f_n(z) = \frac 1{(n - 1)!}( -\log z)^{n - 1}.$$ \emx {Hint}: Show that $Y_k = -\log X_k$ is exponentially distributed.  Usethis to find the density function for $S_n = Y_1 + Y_2 +\cdots+ Y_n$, andfrom this the cumulative distribution and density of $Z_n = e^{-S_n}$.\i\label{exer 7.2.106} Assume that $X_1$ and $X_2$ are independent random variables, eachhaving an exponential density with parameter~$\lambda$.  Show that $Z = X_1 - X_2$ hasdensity$$f_Z(z) = (1/2)\lambda e^{-\lambda |z|}\ .$$\i\label{exer 7.2.107} Suppose we want to test a coin for fairness.  We flip the coin $n$times and record the number of times $X_0$ that the coin turns up tails and thenumber of times $X_1 = n - X_0$ that the coin turns up heads.  Now we set$$Z= \sum_{i = 0}^1 \frac {(X_i - n/2)^2}{n/2}\ .$$Then for a fair coin $Z$ has approximately a chi-squared distribution with $2 -1 = 1$ degree of freedom.  Verify this by computer simulation first for a faircoin ($p~=~1/2$) and then for a biased coin ($p~=~1/3$).\i\label{exer 7.2.108} Verify your answers in Exercise~\ref{exer 7.2.1}(a) by computersimulation: Choose $X$ and $Y$ from $[-1,1]$ with uniform density and calculate$Z = X + Y$.  Repeat this experiment 500 times, recording the outcomes in a bargraph on $[-2,2]$ with 40~bars.  Does the density $f_Z$ calculated in Exercise~\ref{exer7.2.1}(a) describe the shape of your bar graph?  Try this for Exercises~\ref{exer 7.2.1}(b)~and~Exercise~\ref{exer 7.2.1}(c), too.\i\label{exer 7.2.109} Verify your answers to Exercise~\ref{exer 7.2.2} by computersimulation.\i\label{exer 7.2.110} Verify your answer to Exercise~\ref{exer 7.2.3} by computersimulation.\i\label{exer 7.2.18} The  \emx {support} of a function $f(x)$ is defined to be the set$$\{x\ :\ f(x) > 0\}\ .$$Suppose that $X$ and $Y$ are two continuous random variables withdensity functions $f_X(x)$ and $f_Y(y)$, respectively, and suppose that the supports of thesedensity functions are the intervals $[a, b]$ and $[c, d]$, respectively.  Find the support of thedensity function of the random variable $X+Y$.\i\label{exer 7.2.111} Let $X_1$,~$X_2$, \dots,~$X_n$ be a sequence of independent randomvariables, all having a common density function $f_X$ with support $[a,b]$ (seeExercise~\ref{exer 7.2.18}).  Let $S_n = X_1 + X_2 +\cdots+ X_n$, with densityfunction $f_{S_n}$.  Show that the support of~$f_{S_n}$ is the interval$[na,nb]$.   \emx {Hint}: Write $f_{S_n} = f_{S_{n - 1}} * f_X$.  Now useExercise~\ref{exer 7.2.18} to establish the desired result by induction.\i\label{exer 7.2.112} Let $X_1$,~$X_2$, \dots,~$X_n$ be a sequence of independent randomvariables, all having a common density function $f_X$.  Let $A = S_n/n$ betheir average.  Find $f_A$ if\begin{enumerate}\item $f_X(x) = (1/\sqrt{2\pi}) e^{-x^2/2}$ (normal density).\item $f_X(x) = e^{-x}$ (exponential density).\par\noindent \emx {Hint}: Write $f_A(x)$ in terms of $f_{S_n}(x)$.\end{enumerate}\end{LJSItem}}